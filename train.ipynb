{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T18:21:01.107461Z",
     "iopub.status.busy": "2020-10-12T18:21:01.1067Z",
     "iopub.status.idle": "2020-10-12T18:21:02.19541Z",
     "shell.execute_reply": "2020-10-12T18:21:02.194464Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 1.131405,
     "end_time": "2020-10-12T18:21:02.195556",
     "exception": false,
     "start_time": "2020-10-12T18:21:01.064151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb\n",
    "from statistics import harmonic_mean \n",
    "from scipy.stats.mstats import hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pickle ='R:/Yann/riiid/cv1_train.pickle'\n",
    "valid_pickle ='R:/Yann/riiid/cv1_valid.pickle'\n",
    "question_file = 'R:/Yann/riiid/questions.csv'\n",
    "#train_pickle = '../input/riiid-cross-validation-files/cv2_train.pickle'\n",
    "#valid_pickle = '../input/riiid-cross-validation-files/cv2_valid.pickle'\n",
    "#question_file = '../input/riiid-test-answer-prediction/questions.csv'\n",
    "debug = False\n",
    "validaten_flg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcs for user stats with loop\n",
    "def add_user_feats(df, answered_correctly_sum_u_dict, count_u_dict):\n",
    "    acsu = np.zeros(len(df), dtype=np.int32)\n",
    "    cu = np.zeros(len(df), dtype=np.int32)\n",
    "    for cnt,row in enumerate(tqdm(df[['user_id','answered_correctly']].values)):\n",
    "        acsu[cnt] = answered_correctly_sum_u_dict[row[0]]\n",
    "        cu[cnt] = count_u_dict[row[0]]\n",
    "        answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "        count_u_dict[row[0]] += 1\n",
    "    user_feats_df = pd.DataFrame({'answered_correctly_sum_u':acsu, 'count_u':cu})\n",
    "    user_feats_df['answered_correctly_avg_u'] = user_feats_df['answered_correctly_sum_u'] / user_feats_df['count_u']\n",
    "    df = pd.concat([df, user_feats_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def add_user_feats_without_update(df, answered_correctly_sum_u_dict, count_u_dict):\n",
    "    acsu = np.zeros(len(df), dtype=np.int32)\n",
    "    cu = np.zeros(len(df), dtype=np.int32)\n",
    "    for cnt,row in enumerate(df[['user_id']].values):\n",
    "        acsu[cnt] = answered_correctly_sum_u_dict[row[0]]\n",
    "        cu[cnt] = count_u_dict[row[0]]\n",
    "    user_feats_df = pd.DataFrame({'answered_correctly_sum_u':acsu, 'count_u':cu})\n",
    "    user_feats_df['answered_correctly_avg_u'] = user_feats_df['answered_correctly_sum_u'] / user_feats_df['count_u']\n",
    "    df = pd.concat([df, user_feats_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def update_user_feats(df, answered_correctly_sum_u_dict, count_u_dict):\n",
    "    for row in df[['user_id','answered_correctly','content_type_id']].values:\n",
    "        if row[2] == 0:\n",
    "            answered_correctly_sum_u_dict[row[0]] += row[1]\n",
    "            count_u_dict[row[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state():\n",
    "    state = defaultdict(dict)\n",
    "    for user_id in np.sort(train['user_id'].unique()):\n",
    "        state[user_id] = {}\n",
    "    total = len(state.keys())\n",
    "    user_content = train.groupby('user_id')['content_id'].apply(np.array).apply(np.sort).apply(np.unique)\n",
    "    user_attempts = train.groupby(['user_id', 'content_id'])['content_id'].count().astype(np.uint8).groupby('user_id').apply(np.array).values\n",
    "    user_attempts -= 1\n",
    "    for user_id, content, attempt in tqdm(zip(state.keys(), user_content, user_attempts),total=total):\n",
    "        state[user_id] = dict(zip(content, attempt))\n",
    "    del user_content, user_attempts\n",
    "    gc.collect()\n",
    "    \n",
    "    return state\n",
    "\n",
    "def update_state(state, test_df):\n",
    "    attempt = []\n",
    "    for idx, (a, b) in test_df[['user_id', 'content_id']].iterrows():\n",
    "        # check if user exists\n",
    "        if a in state:\n",
    "            # check if user already answered the question, if so update it to a maximum of 10\n",
    "            if b in state[a]:\n",
    "                num = (str(state[a][b])).replace(\"[\",\"\")\n",
    "                num = num.replace(\"]\",\"\")\n",
    "                state[a][b] = min(4,int(num)+1)\n",
    "            # if user did not answered the question already, set the number of attempts to 0\n",
    "            else:\n",
    "                state[a][b] = 0\n",
    "        else:\n",
    "            state[a] =  dict(zip([b],[0]))\n",
    "        # add user data to lists\n",
    "        attempt.append(state[a][b])\n",
    "    \n",
    "    return state, attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-523263f2226e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeld_needed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'row_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'content_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'content_type_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'answered_correctly'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prior_question_elapsed_time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prior_question_had_explanation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeld_needed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#train = train[:9600000]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeld_needed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[1;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;31m# e.g.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read data\n",
    "feld_needed = ['row_id', 'user_id', 'content_id', 'content_type_id', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "train = pd.read_pickle(train_pickle)[feld_needed]\n",
    "#train = train[:9600000]\n",
    "valid = pd.read_pickle(valid_pickle)[feld_needed]\n",
    "#valid = valid[:1200000]\n",
    "if debug:\n",
    "    train = train[:6000000]\n",
    "    valid = valid[:60000]\n",
    "train = train.loc[train.content_type_id == False].reset_index(drop=True)\n",
    "valid = valid.loc[valid.content_type_id == False].reset_index(drop=True)\n",
    "\n",
    "# Attempt No\n",
    "train[\"attempt\"] = 1\n",
    "train[\"attempt\"] = train[[\"user_id\",\"content_id\",\"attempt\"]].groupby([\"user_id\",\"content_id\"])[\"attempt\"].cumsum()\n",
    "train['attempt'].values[train['attempt'].values > 4] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(question_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2451798"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3330796a7d1f46c282e35906416717ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=367339.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state = get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, attempt = update_state(state, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[\"attempt\"] = [i+1 for i in attempt]\n",
    "del attempt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answered correctly average for each content\n",
    "content_df = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean']).reset_index()\n",
    "content_df.columns = ['content_id', 'answered_correctly_avg_c']\n",
    "train = pd.merge(train, content_df, on=['content_id'], how=\"left\")\n",
    "valid = pd.merge(valid, content_df, on=['content_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user stats features with loops\n",
    "answered_correctly_sum_u_dict = defaultdict(int)\n",
    "count_u_dict = defaultdict(int)\n",
    "train = add_user_feats(train, answered_correctly_sum_u_dict, count_u_dict)\n",
    "valid = add_user_feats(valid, answered_correctly_sum_u_dict, count_u_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with mean value for prior_question_elapsed_time\n",
    "# note that `train.prior_question_elapsed_time.mean()` dose not work!\n",
    "# please refer https://www.kaggle.com/its7171/can-we-trust-pandas-mean for detail.\n",
    "prior_question_elapsed_time_mean = train.prior_question_elapsed_time.dropna().values.mean()\n",
    "train['prior_question_elapsed_time'] = train.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n",
    "valid['prior_question_elapsed_time'] = valid.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n",
    "\n",
    "# Calculate Harmonic Mean between User and Content Avgs\n",
    "train['hmean'] = 2 / ((1/train.answered_correctly_avg_c)+(1/train.answered_correctly_avg_u))\n",
    "valid['hmean'] = 2 / ((1/valid.answered_correctly_avg_c)+(1/valid.answered_correctly_avg_u))\n",
    "\n",
    "# use only last 30M training data for limited memory on kaggle env.\n",
    "#train = train[-30000000:]\n",
    "\n",
    "# part\n",
    "questions_df = pd.read_csv(question_file)\n",
    "train = pd.merge(train, questions_df[['question_id', 'part']], left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "valid = pd.merge(valid, questions_df[['question_id', 'part']], left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "\n",
    "# changing dtype to avoid lightgbm error\n",
    "train['prior_question_had_explanation'] = train.prior_question_had_explanation.fillna(False).astype('int8')\n",
    "valid['prior_question_had_explanation'] = valid.prior_question_had_explanation.fillna(False).astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "TARGET = 'answered_correctly'\n",
    "FEATS = ['answered_correctly_avg_u', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c', 'part', 'prior_question_elapsed_time', 'hmean', 'attempt']\n",
    "dro_cols = list(set(train.columns) - set(FEATS))\n",
    "y_tr = train[TARGET]\n",
    "y_va = valid[TARGET]\n",
    "train.drop(dro_cols, axis=1, inplace=True)\n",
    "valid.drop(dro_cols, axis=1, inplace=True)\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train[FEATS], y_tr)\n",
    "lgb_valid = lgb.Dataset(valid[FEATS], y_va)\n",
    "del train, y_tr\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(\n",
    "                    {'objective': 'binary',\n",
    "                    'metric': 'auc',\n",
    "                    'seed' : 11}, \n",
    "                    lgb_train,\n",
    "                    valid_sets=[lgb_train, lgb_valid],\n",
    "                    verbose_eval=100,\n",
    "                    num_boost_round=10000,\n",
    "                    early_stopping_rounds=10\n",
    "                )\n",
    "print('auc:', roc_auc_score(y_va, model.predict(valid[FEATS])))\n",
    "_ = lgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(\"lgbmodel_cv2.pickle\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
